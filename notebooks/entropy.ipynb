{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy is the \"average information\" or \"surprisal\" received from a random variable. \n",
    "\n",
    "Requirements for the definition of surprisal $h$ of a random variable value $x$:\n",
    "\n",
    "- Decreasing function of probability (a value with a small probability is very surprising). The information gained from the occurence of an event can be thought as \"moving the probability mass\". The more unexpected the event, the larger the information gain on the underlying probability distribution.\n",
    "- Additive for independent events (\"If I roll a die and flip a coin, my total information should be some additive combination of the two probabilities.\") â†’ $h(x, y)=h(x)+h(y)$ where $p(x, y) = p(x)p(y)$\n",
    "\n",
    "As a decreasing function that turns products into sums we can use: $-\\log$\n",
    "\n",
    "Natural choice: entropy $H$ of a random variable $X$ is the average surprisal  \n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb E [h(x)] = \\mathbb E [-\\log(p(x))].\n",
    "$$\n",
    "\n",
    "The entropy is also the average number of bits needed to encode information from a probabilistic source.\n",
    "\n",
    "## Exercise: optimal encoding\n",
    "\n",
    "What is the entropy of a morse codes distribution (i.e. sequences of `\".\"`, `\"-\"`, and `\" \"`), if `p(\".\")=p(\"-\")=p(\" \")=1/3`?\n",
    "\n",
    "What if the individual character probabilities are the following: \n",
    "\n",
    "`p(\".\")=p(\"-\")=1/4`, and `p(\" \")=1/2`?\n",
    "\n",
    "Apply your encoding scheme to the following random sample, and compute the corresponding entropy (average number of bits per character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ' ', '.', ' ', '-', '-', ' ', ' ', ' ', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample_from_distribution(n_samples=100):\n",
    "    choices = [\".\", \"-\", \" \"]\n",
    "    probabilities = [0.25, 0.25, 0.5]\n",
    "    return random.choices(choices, probabilities, k=n_samples)\n",
    "\n",
    "sample_from_distribution(10)\n",
    "\n",
    "# TODO: compute the entropy of this distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "Cross-entropy between a distribution $P$ (typically, the real data distribution) and a distribution $Q$ (typically, a fitted model) is the expected value of $-\\log Q$ over the real distribution from which we have samples:\n",
    "\n",
    "$$ \n",
    "H(P, Q) = \\mathbb E_{x\\sim P} \\left[-\\log(Q(x)) \\right]\n",
    "$$\n",
    "\n",
    "Alternatively, the cross-entropy between $P$ and $Q$ can be viewed as the sum of entropy of the real data distribution $P$ and the Kullback-Leibler divergence between $P$ and the estimated \n",
    "distribution $Q$\n",
    "\n",
    "$$\n",
    "H(P, Q)=H(P)+D_{K L}(P \\| Q)\n",
    "$$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "If the real data distribution is given by `p(\".\")=p(\"-\")=1/4`, and `p(\" \")=1/2`, and the estimated data distribution is `q(\".\")=q(\"-\")=q(\" \")=1/3`, what is the cross-entropy?\n",
    "\n",
    "What if $q$ estimates $p$ perfectly?\n",
    "\n",
    "---\n",
    "\n",
    "More accurate predictive models can reduce the entropy. E.g. if the dataset is `.- .- .-` a simple (unigram) model would estimate `q(\".\")=q(\"-\")=q(\" \")=1/3`. A bigram model $q(x_2|x_1)$ would give a more accurate estimate (which one?), and reduce the entropy to ... (exercise ðŸ˜Š)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Perplexity is a convenient, interpretable equivalent of cross-entropy. It reads\n",
    "\n",
    "$$\n",
    "\\text{PPL}(P, Q) = e^{H(P, Q)}\n",
    "$$\n",
    "\n",
    "and tells us that the model $Q$ has a $1/\\text{PPL}(P, Q)$ chance of predicting the next token correctly.\n",
    "\n",
    "## Bits-per-byte\n",
    "\n",
    "Since entropy is computed per input token, longer tokens tend to have higher entropy, so models using longer tokens are at a disadvantage. Bits-per-byte mitigates this inequity by dividing entropy by the average number of bytes needed to encode a token.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "What are the perplexity and bits-per-byte of the morse code model above (`p(\".\")=p(\"-\")=1/4`, `p(\" \")=1/2`, `q(\".\")=q(\"-\")=q(\" \")=1/3`)?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
